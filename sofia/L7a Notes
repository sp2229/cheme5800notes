
Dimensionality Reduction
- Covariance matrix
- Singular value decomposition: any array can be factored to create approximations of original arrays (low rank approximations/ data compression/ solivng non-square systems)

Dimensionality Reduction Problem 
- Dataset D= {x1,x2,...,xn} where xi is an m-dimensional feature vector
- M features compressed to K 
- Example is compressing genes to key patterns 
- Megapixel image 
- Netflix movie recommendation system 

Composite Features
- y vector is lower dimension composite feature vector 
    y=P(x-x*)
    x* is the mean, y is new composite vector, P is transformation matrix P (kxm) where P= [ø1,...,øk]
- Principle component analysis is reduction procedure to find top k-eigenvectors 

Covariance
- Covariance matrix is sigma, sigma hat is empirical (data estimate)
- Sum over all samples of how feature i deviates from the mean of that feature times deviation from the mean of feature j
- sigma = stdev. i * stdev. j * correlation 
- correlation is 0 if not relates, 1 if they move in opposite directions, 1 if they move in the same direction
- diagonal (i and i) correlation is 1 

X= 
- n rows of feature 1 (column 1), each feature is a column
- Samples are along rows 
- Center data by subtracting the mean of each feature and subtract them off (m=[x*1, x*2, ...]; m is the vector with the mean of each feature)
    - m is an outer product (ab)ij = aibj
    - a is n ;  b is m 

Covariance indicates strength + relationship of feature i and j 
- >0 positively correlated 
- <0 negative correlation 
- sigma (ij) = sigma (ji) means it is symmetric (because it's a product)

Positive semi-definite: 
- any vector v that is transposed and multiplied by the covariance v ≥ 0


Decomposing the covariance matrix 
- real, symmetric, positive semi-definite (0), eigen pairs are
    1) orthogonal, can be scaled to be orthonormal, dot product is 1
    2) if covariance of i ≠ j is 0; i=j is 1 
    3) v is eigenvectors with orthonormal columns; vT is the transpose (vTv=Identity Matrix)
    4) n = diagn (l1,...,lm) where l1≥l2>...>lm≥0
        l1 is the variance captured by principal component v1 (v1 is the greatest variance)
    5) yi=viT(x-x*) --> for each row, mean is subtracted and each eigenvector gives the ith component 


original mxn matrix reduced to kxn matrix 
(1) use intermediate matrix covariance
- HAS TO BE CENTERED DATA MATRIX 

(2) SVD is a factorization for a rectangular matrix --> gives same principal components but 
- A= U S VT
- U is orthongal (UUT= I)
- S is rectangular diagonal
- V is orthogonal (VVT = I)
- rank is # of unique pieces of information in an array 
- A = Sumn of rank of singular values 